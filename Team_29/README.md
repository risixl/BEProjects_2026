# Multi-Modal AI Assistant for Visually Impaired

A comprehensive voice-controlled assistive technology system integrating multiple AI services for currency recognition, face identification, OCR, scene description, and smart navigation support.

## ðŸŽ¯ Project Overview

This system combines artificial intelligence and sensor-based navigation to enhance mobility and independence for visually impaired users. It features a sophisticated multi-modal AI agent that intelligently routes voice commands to specialized services, providing real-time environmental awareness and object recognition.

## âœ¨ Key Features

### ðŸ¤– Multi-Modal AI Services
- **Currency Recognition**: Identify currency denominations using Gemini Vision API (94.5% accuracy)
- **Face Recognition**: Detect and identify people using InsightFace with SQLite database (89.2% accuracy)
- **OCR Text Reading**: Extract text from images using Gemini 2.0 Flash API
- **Scene Description**: Comprehensive environmental understanding with SmolVLM2-2.2B
- **Enhanced Scene Analysis**: Personalized scene description with automatic face recognition
- **General Q&A**: Conversational AI assistance using Gemini Flash 2.0

### ðŸŽ¤ Voice Interface
- Continuous voice recognition with background listening
- Natural language intent classification using Mistral API
- ElevenLabs text-to-speech for high-quality audio output
- Fallback to pyttsx3 for offline TTS

### ðŸš§ Smart Navigation (Smart Stick)
- Ultrasonic obstacle detection (HC-SR04 sensor)
- Water level detection 
- Fire/smoke detection 
- Coordinated voice alerts
- 

### ðŸ”€ Intelligent Routing
- LangGraph-based workflow orchestration
- Mistral Small API for intent classification (93.3% accuracy)
- State management and conditional routing
- Error recovery and graceful fallbacks

## ðŸ—ï¸ System Architecture

```
Voice Input â†’ Intent Router (Mistral) â†’ Service Selection â†’ Processing â†’ Audio Output
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“               â†“               â†“           â†“          â†“
  OCR          Currency         Face        Scene      Gemini
Service         Recog.         Recog.       Desc.       Q&A
    â†“               â†“               â†“           â†“          â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
           Smart Navigation Module
           (Obstacle/Water/Fire Sensors)
                    â†“
           TTS Output (ElevenLabs/pyttsx3)
```

## ðŸ“‹ Prerequisites

### Hardware Requirements
- **Computer/Laptop**: Intel Core i5 or equivalent, 8GB RAM minimum
- **Raspberry Pi 4**: Model B, 4GB RAM (for deployment)
- **Webcam**: 720p minimum resolution
- **Microphone**: For voice input
- **Speakers/Headphones**: For audio output

### Sensor Hardware (Smart Stick Module)
- HC-SR04 Ultrasonic Distance Sensor
- Capacitive Water Level Sensor
- MQ-2 Smoke/Gas Sensor
- Active Buzzer Module
- Jumper wires and breadboard/raspberry pi 

### Software Requirements
- Python 3.9 or higher
- pip (Python package manager)
- Git (for cloning repository)

## ðŸš€ Installation

### 1. Clone the Repository
```bash
git clone https://github.com/Kiran212004/unified-multimodal-intelligence-to-blinds.git
cd unified-multimodal-intelligence-to-blinds
```

### 2. Create Virtual Environment (Recommended)
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Install Additional Packages
```bash
# Core AI/ML Libraries
pip install insightface opencv-python numpy paddleocr imgaug

# LangChain & LangGraph
pip install langgraph langchain-core pydantic

# Voice & Audio
pip install SpeechRecognition pyaudio pyttsx3 elevenlabs

# Web & API
pip install requests python-dotenv

# Image Processing
pip install Pillow
```

### 5. Configure Environment Variables

Create a `.env` file in the project root:

```env
# API Keys
ELEVEN_LABS_API_KEY=your_elevenlabs_api_key_here
MISTRAL_API_KEY=your_mistral_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here

# Model Configuration
MISTRAL_MODEL_NAME=mistral-small-latest
VLM_URL=http://localhost:1234
```

### 6. Set Up Face Database
```bash
# The database will be created automatically on first run
# faces.db will store facial embeddings
```

## ðŸ“ Project Structure

```
currency_recognition/
â”œâ”€â”€ agent.py                    # Main multi-modal AI agent
â”œâ”€â”€ currency.py                 # Currency recognition (Gemini API)
â”œâ”€â”€ OCR.py                      # OCR text reading (Gemini API)
â”œâ”€â”€ vision.py                   # Scene description (SmolVLM)
â”œâ”€â”€ scene_describe.py           # Enhanced scene with face recognition
â”œâ”€â”€ sensor.py                   # Smart stick sensor integration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Environment variables (create this)
â”œâ”€â”€ faces.db                    # Face recognition database (auto-created)
â”œâ”€â”€ temp/                       # Temporary image storage
â”œâ”€â”€ augmented_dataset/          # Currency training dataset
â”œâ”€â”€ face/
â”‚   â”œâ”€â”€ face_recognition_voice.py  # Face recognition service
â”‚   â”œâ”€â”€ face_detection.py          # Face detection utilities
â”‚   â””â”€â”€ facedbcheck.py             # Database management
â””â”€â”€ docs/
    â””â”€â”€ about.md
```

## ðŸŽ® Usage

### Running the Main Agent

```bash
python agent.py
```

The system will initialize and wait for voice commands.

### Voice Commands

#### General Interaction
- **"Start"** - Begin any service
- **"Stop"** / **"Exit"** / **"Quit"** - Exit the system

#### Service-Specific Commands
- **"Can you read this text?"** â†’ Routes to OCR service
- **"What currency is this?"** â†’ Routes to currency recognition
- **"Who is this person?"** â†’ Routes to face recognition
- **"What do you see?"** â†’ Routes to scene description
- **"What's the weather?"** â†’ Routes to Gemini Q&A agent

### Individual Service Modules

#### 1. Currency Recognition
```bash
python currency.py
```
- Say "start" to capture and identify currency
- System will announce denomination

#### 2. Face Recognition
```bash
cd face
python face_recognition_voice.py
```
- **Train mode**: Say "train" to register a new face
- **Identify mode**: Say "identify" to recognize a face

#### 3. Enhanced Scene Description
```bash
python scene_describe.py
```
- Combines scene understanding with face recognition
- Announces recognized people by name with their activities

#### 4. Smart Stick Sensor System
```bash
# On Raspberry Pi
python sensor.py
```
- Automatically monitors for obstacles, water, and fire
- Provides voice alerts and buzzer patterns

## ðŸ”§ Configuration

### Adjusting Voice Recognition Sensitivity
Edit in respective files:
```python
# In agent.py or service files
audio = r.listen(source, timeout=5, phrase_time_limit=5)
```

### Modifying TTS Speed
```python
# In agent.py
engine.setProperty('rate', 150)  # Adjust speech speed
engine.setProperty('volume', 1.0)  # Adjust volume
```

### Changing Face Recognition Threshold
```python
# In face/face_recognition_voice.py
if similarity > 0.5:  # Adjust threshold (0.0 to 1.0)
```

### Sensor Alert Cooldown
```python
# In sensor.py
alert_cooldown = 3  # Seconds between voice alerts
```

## ðŸŽ¯ Workflow

### 1. System Initialization
```
Start Agent â†’ Load Models â†’ Initialize Voice Interface â†’ Ready State
```

### 2. Voice Command Processing
```
User Speaks â†’ Speech Recognition â†’ Intent Classification (Mistral) â†’ Route to Service
```

### 3. Service Execution Flow

#### Currency Recognition
```
Voice Command â†’ Image Capture (3s countdown) â†’ Gemini API â†’ Clean Response â†’ TTS Output
```

#### Face Recognition
```
Voice Command â†’ Mode Selection (train/identify) â†’ Image Capture â†’ 
InsightFace Embedding â†’ Database Query â†’ Cosine Similarity â†’ TTS Output
```

#### Scene Description
```
Voice Command â†’ Image Capture â†’ Face Detection â†’ Scene Analysis (SmolVLM) â†’ 
Enhancement (Gemini) â†’ Personalized Description â†’ TTS Output
```

#### Smart Navigation
```
Continuous Monitoring â†’ Sensor Reading â†’ Hazard Detection â†’ 
Priority Classification â†’ Buzzer Pattern + Voice Alert
```

## ðŸ§ª Testing

### Test Individual Modules
```bash
# Test webcam
python face/webcam_test.py

# Test face detection
python face/face_detection.py

# Test database
python face/facedbcheck.py

# Delete face database (if needed)
python delete_faces.py
```

### Performance Benchmarks
- **Currency Recognition**: 92.1% accuracy, ~2.1s response time
- **Face Recognition**: 89.2% accuracy, ~2.3s response time
- **OCR**: 94.5% character accuracy, ~3.8s response time
- **Scene Description**: 91.3% completeness, ~4.6s response time
- **Voice Recognition**: 87.8% accuracy, ~1.2s latency

## ðŸ“Š Model Information

### AI Models Used
1. **Currency Recognition**: Gemini 2.0 Flash (Google)
2. **Face Detection/Recognition**: InsightFace Buffalo_L
3. **OCR**: Gemini Vision API
4. **Scene Description**: SmolVLM2-2.2B-Instruct
5. **Intent Router**: Mistral Small Latest
6. **Q&A Agent**: Gemini Flash 2.0

### Database
- **Face Storage**: SQLite with 512-dimensional embeddings
- **Similarity Metric**: Cosine similarity (threshold: 0.5)


### API Errors
- Verify API keys in `.env` file
- Check internet connectivity
- Confirm API quota limits

### NumPy Version Issues
The project includes a compatibility patch for NumPy 2.0. If you encounter NumPy-related errors:
```bash
pip install "numpy<2.0.0"
```


## ðŸŒ Deployment

### Raspberry Pi Deployment
1. Install Raspberry Pi OS (64-bit)
2. Install Python 3.9+
3. Install dependencies (use `tflite-runtime` instead of full TensorFlow)
4. Configure GPIO pins for sensors
5. Set up autostart script

```bash
# Create systemd service
sudo nano /etc/systemd/system/voice-assistant.service
```

## ðŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ðŸ“„ License

This project is part of research work for assistive technology development.


## ðŸ™ Acknowledgments

- InsightFace for face recognition models
- Google for Gemini Vision API
- Mistral AI for routing models
- ElevenLabs for text-to-speech
- Open-source community for various libraries


## ðŸ”® Future Enhancements

- [ ] Multi-language support
- [ ] Offline mode with edge models
- [ ] GPS integration for outdoor navigation
- [ ] Medication recognition
- [ ] Product barcode scanning
- [ ] IoT smart home integration
- [ ] Mobile app development
- [ ] Cloud synchronization for face database

---


